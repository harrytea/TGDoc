// {
//     // Normal DDP
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "python",
//             "request": "launch",
//             "program": "/home/wangyh/miniconda3/envs/llava2/lib/python3.10/site-packages/torch/distributed/run.py",
//             "console": "integratedTerminal",
//             "justMyCode": true,
//             // "args":[
//             //     "--nproc_per_node=1",
//             //     "--master_port=7421",
//             //     "llava/train/train_mem.py",
//             //     "--model_name_or_path", "/data/wangyh/mllms/LLaVA/checkpoints/vicuna-7b-v1.5",
//             //     "--version", "v1",
//             //     "--data_path", "/data/wangyh/mllms/LLaVA/datasets/instruction",
//             //     "--image_folder", "/data/wangyh/mllms/LLaVA/datasets/images",
//             //     "--vision_tower", "openai/clip-vit-large-patch14",
//             //     "--pretrain_mm_mlp_adapter", "/data/wangyh/mllms/LLaVA_0721_work/checkpoints_work_after_reconfig/llava-7b-pretrain/mm_projector.bin",
//             //     "--mm_vision_select_layer", "-2",
//             //     "--mm_use_im_start_end", "True",
//             //     "--bf16", "True",
//             //     "--output_dir", "./checkpoints/llava-7b-finetune",
//             //     "--num_train_epochs", "2",
//             //     "--per_device_train_batch_size", "4",
//             //     "--per_device_eval_batch_size", "4",
//             //     "--gradient_accumulation_steps", "1",
//             //     "--evaluation_strategy", "no",
//             //     "--save_strategy", "steps",
//             //     "--save_steps", "50000",
//             //     "--save_total_limit", "1",
//             //     "--learning_rate", "2e-5",
//             //     "--weight_decay", "0.",
//             //     "--warmup_ratio", "0.03",
//             //     "--lr_scheduler_type", "cosine",
//             //     "--logging_steps", "1",
//             //     "--tf32", "True",
//             //     "--fsdp", "full_shard auto_wrap",
//             //     "--fsdp_transformer_layer_cls_to_wrap", "LlamaDecoderLayer",
//             //     "--model_max_length", "2048",
//             //     "--gradient_checkpointing", "True",
//             //     "--dataloader_num_workers", "4",
//             //     "--lazy_preprocess", "True",
//             //     "--report_to", "none",
//             // ],

//             "args":[
//                 "--nproc_per_node=1",
//                 "--master_port=19723",
//                 "llava/train/train_mem.py",
//                 "--model_name_or_path", "/data/wangyh/mllms/LLaVA/checkpoints/vicuna-7b-v1.5",
//                 "--version", "v1",
//                 "--data_path", "/data/wangyh/mllms/LLaVA/datasets/instruction",  // 所有instruction的路径
//                 "--image_folder", "/data/wangyh/mllms/LLaVA/datasets/images",  // 所有image的路径
//                 "--data_stage", "pretrain",
//                 "--vision_tower", "openai/clip-vit-large-patch14",
//                 "--tune_mm_mlp_adapter", "True",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end",
//                 "--bf16", "True",
//                 "--output_dir", "./checkpoints/llava-7b-finetune",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "16",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "1",
//                 "--evaluation_strategy", "no",
//                 "--save_strategy", "steps",
//                 "--save_steps", "24000",
//                 "--save_total_limit", "1",
//                 "--learning_rate", "2e-3",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "2048",
//                 "--gradient_checkpointing", "True",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "none",
//             ],
//             "env": {"CUDA_VISIBLE_DEVICES": "7"},
//         }
//     ]
// }



// {
//     // DeepSpeed
//     "version": "0.2.0",
//     "configurations": [
//         {
//             "name": "Python: Current File",
//             "type": "python",
//             "request": "launch",
//             "program": "/home/wangyh/miniconda3/envs/llava/bin/deepspeed",
//             "console": "integratedTerminal",
//             "justMyCode": true,
//             // pretrain
//             "args":[
//                 "--include", "localhost:5",
//                 "--master_port=23413",
//                 "llava/train/train_mem.py",
//                 "--deepspeed", "/data/wangyh/mllms/LLaVA_grounding_lora/scripts/zero2.json",
//                 "--model_name_or_path", "/data/wangyh/mllms/LLaVA/checkpoints/vicuna-7b-v1.5",
//                 "--version", "v1",
//                 "--data_path", "/data/wangyh/mllms/LLaVA/datasets/instruction",  // 所有instruction的路径
//                 "--image_folder", "/data/wangyh/mllms/LLaVA/datasets/images",  // 所有image的路径
//                 "--data_stage", "pretrain",
//                 "--vision_tower", "openai/clip-vit-large-patch14",
//                 "--tune_mm_mlp_adapter", "True",
//                 "--mm_vision_select_layer", "-2",
//                 "--mm_use_im_start_end",
//                 "--bf16", "True",
//                 "--output_dir", "./checkpoints/llava-7b-pretrain",
//                 "--num_train_epochs", "1",
//                 "--per_device_train_batch_size", "16",
//                 "--per_device_eval_batch_size", "4",
//                 "--gradient_accumulation_steps", "1",
//                 "--evaluation_strategy", "no",
//                 "--save_strategy", "steps",
//                 "--save_steps", "24000",
//                 "--save_total_limit", "1",
//                 "--learning_rate", "2e-3",
//                 "--weight_decay", "0.",
//                 "--warmup_ratio", "0.03",
//                 "--lr_scheduler_type", "cosine",
//                 "--logging_steps", "1",
//                 "--tf32", "True",
//                 "--model_max_length", "2048",
//                 "--gradient_checkpointing", "True",
//                 "--lazy_preprocess", "True",
//                 "--report_to", "none",
//             ],
//             // // finetune
//             // "args":[
//             //     "--include", "localhost:5",
//             //     "--master_port=12383",
//             //     "llava/train/train_mem.py",
//             //     "--deepspeed", "/data/wangyh/mllms/LLaVA_grounding_lora/scripts/zero3.json",
//             //     "--model_name_or_path", "/data/wangyh/mllms/LLaVA/checkpoints/vicuna-7b-v1.5",
//             //     "--version", "v1",
//             //     "--data_path", "/data/wangyh/mllms/LLaVA/datasets/instruction",
//             //     "--image_folder", "/data/wangyh/mllms/LLaVA/datasets/images",
//             //     "--data_stage", "finetune",
//             //     "--vision_tower", "openai/clip-vit-large-patch14",
//             //     "--pretrain_mm_mlp_adapter", "/data/wangyh/mllms/LLaVA_grounding_lora/checkpoints/llava-7b-pretrain/mm_projector.bin",
//             //     "--mm_vision_select_layer", "-2",
//             //     "--mm_use_im_start_end", "True",
//             //     "--bf16", "True",
//             //     "--output_dir", "./checkpoints/llava-7b-finetune",
//             //     "--num_train_epochs", "1",
//             //     "--per_device_train_batch_size", "4",
//             //     "--per_device_eval_batch_size", "4",
//             //     "--gradient_accumulation_steps", "1",
//             //     "--evaluation_strategy", "no",
//             //     "--save_strategy", "steps",
//             //     "--save_steps", "50000",
//             //     "--save_total_limit", "1",
//             //     "--learning_rate", "2e-3",
//             //     "--weight_decay", "0.",
//             //     "--warmup_ratio", "0.03",
//             //     "--lr_scheduler_type", "cosine",
//             //     "--logging_steps", "1",
//             //     "--tf32", "True",
//             //     "--model_max_length", "2048",
//             //     "--gradient_checkpointing", "True",
//             //     "--dataloader_num_workers", "4",
//             //     "--lazy_preprocess", "True",
//             //     "--report_to", "none",
//             // ],
//             // "env": {"CUDA_VISIBLE_DEVICES": "6"},
//         }
//     ]
// }


{
    // inference
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args":[
                "--model-path", "./checkpoints/llava-7b-finetune",
                "--image-file", "/data/wangyh/mllms/LLaVA_grounding_lora/watch.jpg",
                "--conv-mode", "llava_v1",
            ],
            "env": {
                "PYTHONPATH":"${workspaceFolder}",
                "CUDA_VISIBLE_DEVICES": "5",
            },
        }
    ]
}